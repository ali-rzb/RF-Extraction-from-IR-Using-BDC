{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 5s 154ms/step382/ 382 - ETA :   0m  0s - 62 step/s - passed\n",
      "12/12 [==============================] - 3s 161ms/step360/ 360 - ETA :   0m  0s - 66 step/s - passed\n",
      "11/11 [==============================] - 4s 265ms/step336/ 336 - ETA :   0m  0s - 66 step/s - passed\n",
      "12/12 [==============================] - 2s 56ms/step 382/ 382 - ETA :   0m  0s - 66 step/s - pass\n",
      "12/12 [==============================] - 2s 43ms/step 360/ 360 - ETA :   0m  0s - 62 step/s - passed t\n",
      "11/11 [==============================] - 2s 56ms/step 336/ 336 - ETA :   0m  0s - 62 step/s - passed t\n",
      "12/12 [==============================] - 2s 72ms/step 382/ 382 - ETA :   0m  0s - 66 step/s - passed\n",
      "12/12 [==============================] - 2s 69ms/step 360/ 360 - ETA :   0m  0s - 62 step/s - passed\n",
      "11/11 [==============================] - 2s 59ms/step 336/ 336 - ETA :   0m  0s - 62 step/s - passed t\n",
      "12/12 [==============================] - 2s 46ms/step 382/ 382 - ETA :   0m  0s - 66 step/s - passed\n",
      "12/12 [==============================] - 2s 60ms/step 360/ 360 - ETA :   0m  0s - 62 step/s - passe\n",
      "11/11 [==============================] - 2s 72ms/step 336/ 336 - ETA :   0m  0s - 66 step/s - passed t\n",
      "12/12 [==============================] - 2s 62ms/step 382/ 382 - ETA :   0m  0s - 62 step/s - passed\n",
      "12/12 [==============================] - 2s 54ms/step 360/ 360 - ETA :   0m  0s - 62 step/s - passed\n",
      "11/11 [==============================] - 2s 43ms/step 336/ 336 - ETA :   0m  0s - 66 step/s - passed t\n",
      "12/12 [==============================] - 2s 35ms/step 382/ 382 - ETA :   0m  0s - 66 step/s - passed t\n",
      "12/12 [==============================] - 2s 35ms/step 360/ 360 - ETA :   0m  0s - 62 step/s - passed\n",
      "11/11 [==============================] - 2s 63ms/step 336/ 336 - ETA :   0m  0s - 66 step/s - passed\n",
      "12/12 [==============================] - 2s 44ms/step 382/ 382 - ETA :   0m  0s - 66 step/s - passed\n",
      "12/12 [==============================] - 1s 24ms/step 360/ 360 - ETA :   0m  0s - 66 step/s - passed time \n",
      "11/11 [==============================] - 2s 51ms/step 336/ 336 - ETA :   0m  0s - 66 step/s - passed t\n",
      "12/12 [==============================] - 2s 50ms/step 382/ 382 - ETA :   0m  0s - 62 step/s - passed\n",
      "12/12 [==============================] - 2s 59ms/step 360/ 360 - ETA :   0m  0s - 62 step/s - passed\n",
      "11/11 [==============================] - 1s 24ms/step 336/ 336 - ETA :   0m  0s - 62 step/s - passed time \n",
      "12/12 [==============================] - 2s 39ms/step 382/ 382 - ETA :   0m  0s - 62 step/s - passed tim\n",
      "12/12 [==============================] - 2s 62ms/step 360/ 360 - ETA :   0m  0s - 66 step/s - passed\n",
      "11/11 [==============================] - 2s 34ms/step 336/ 336 - ETA :   0m  0s - 62 step/s - passed t\n",
      "12/12 [==============================] - 1s 25ms/step 382/ 382 - ETA :   0m  0s - 62 step/s - passed tim\n",
      "12/12 [==============================] - 1s 25ms/step 360/ 360 - ETA :   0m  0s - 62 step/s - passed tim\n",
      "11/11 [==============================] - 2s 63ms/step 336/ 336 - ETA :   0m  0s - 62 step/s - passed t\n",
      "12/12 [==============================] - 5s 186ms/step382/ 382 - ETA :   0m  0s - 66 step/s - passed\n",
      "12/12 [==============================] - 3s 215ms/step360/ 360 - ETA :   0m  0s - 62 step/s - pass\n",
      "11/11 [==============================] - 4s 247ms/step336/ 336 - ETA :   0m  0s - 62 step/s - passed\n",
      "12/12 [==============================] - 2s 37ms/step 382/ 382 - ETA :   0m  0s - 62 step/s - passed\n",
      "12/12 [==============================] - 2s 60ms/step 360/ 360 - ETA :   0m  0s - 55 step/s - passed\n",
      "11/11 [==============================] - 2s 53ms/step 336/ 336 - ETA :   0m  0s - 62 step/s - pass\n",
      "12/12 [==============================] - 2s 54ms/step 382/ 382 - ETA :   0m  0s - 62 step/s - passed\n",
      "12/12 [==============================] - 2s 68ms/step 360/ 360 - ETA :   0m  0s - 62 step/s - passed\n",
      "11/11 [==============================] - 2s 64ms/step 336/ 336 - ETA :   0m  0s - 62 step/s - passe\n",
      "12/12 [==============================] - 2s 66ms/step 382/ 382 - ETA :   0m  0s - 66 step/s - passed\n",
      "12/12 [==============================] - 2s 53ms/step 360/ 360 - ETA :   0m  0s - 62 step/s - passed\n",
      "11/11 [==============================] - 1s 26ms/step 336/ 336 - ETA :   0m  0s - 62 step/s - passed t\n",
      "12/12 [==============================] - 1s 36ms/step 382/ 382 - ETA :   0m  0s - 66 step/s - passed\n",
      "12/12 [==============================] - 1s 28ms/step 360/ 360 - ETA :   0m  0s - 66 step/s - passed t\n",
      "11/11 [==============================] - 2s 64ms/step 336/ 336 - ETA :   0m  0s - 62 step/s - passed t\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import Functions.GlobalUtils as g_utils\n",
    "import Functions.DataUtils as d_utils\n",
    "import Functions.VideoUtils as v_utils\n",
    "import DATA_INFO as data_info\n",
    "import Functions.LocalDB as db\n",
    "import numpy as np\n",
    "from Functions.CNN import CNN\n",
    "from tensorflow import keras\n",
    "import os\n",
    "\n",
    "run_name = ''\n",
    "folds = range(5)\n",
    "\n",
    "class_num = 10\n",
    "augment_ratio = 2\n",
    "\n",
    "data_folder_name = 'T{}.C{}.V{}.A{}/'.format(1,class_num,20, augment_ratio)\n",
    "labeled_data_info = db.get_db(data_info.Labeled.path + data_folder_name+'data_info.txt', d_utils.Labeled_data_info)\n",
    "f_max, f_min = d_utils.get_data_max_min(data_info.Trimmed.path, 'flow', '{:02d}data.txt')\n",
    "\n",
    "test_indexes = labeled_data_info[0].test_videos_indexes\n",
    "folder = 'Predictions/'\n",
    "\n",
    "_borders = labeled_data_info[0].classes_borders\n",
    "_borders[0] = 0\n",
    "\n",
    "models = [\"DenseNet121\", \"EfficientNetB0\", \"InceptionV3\"]\n",
    "for model_name in models:\n",
    "    if not os.path.isdir(f\"{folder}/{model_name}\"):\n",
    "        os.mkdir(f\"{folder}/{model_name}\")\n",
    "    for fold in folds:\n",
    "        for i in test_indexes:\n",
    "            data_X, data_Y, real_flow = d_utils.single_vid_Data_Labeling(i, class_num, borders = _borders, resize_image = (75,75), gray_scale=False)\n",
    "\n",
    "            # model_name = \"DenseNet121\"\n",
    "            _model_name = f\"F{fold}_C10_{model_name}.h5\"\n",
    "            full_model_name = f'models/{model_name}/{_model_name}'\n",
    "            model = keras.models.load_model(full_model_name, custom_objects={\"f1_score\": g_utils.f1_score()})\n",
    "\n",
    "            raw_pred = model.predict(data_X)\n",
    "            _predicted = list(np.argmax(raw_pred, axis=-1))\n",
    "            \n",
    "                        \n",
    "            db.insert(f'{folder}/{model_name}/{_model_name}.predict{run_name}.txt', CNN.prediction_data(\n",
    "                raw_pred    = [list(x) for x in raw_pred],\n",
    "                predicted   = _predicted, \n",
    "                class_num   = class_num,\n",
    "                truth_class = data_Y, \n",
    "                truth_flow  = real_flow, \n",
    "                borders     = _borders,\n",
    "                vid_name    = data_info.Synced.FileNames.video_name.format(i) + '.mp4',\n",
    "                model_name  = full_model_name,\n",
    "                f_max       = f_max,\n",
    "                f_min       = f_min))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0   64  262  {0: 64, 1: 70, 4: 62, 2: 64, 3: 66}\n",
      "382    0    0  {-1: 382}\n",
      "  0   80  319  {2: 89, 1: 74, 0: 80, 4: 76, 3: 80}\n",
      "  0   81  308  {4: 94, 3: 71, 0: 81, 1: 72, 2: 71}\n",
      "  0   70  304  {0: 70, 2: 79, 4: 77, 3: 67, 1: 81}\n",
      "  0   70  297  {3: 71, 1: 76, 4: 71, 0: 70, 2: 79}\n",
      "  0   74  302  {1: 71, 3: 91, 0: 74, 4: 76, 2: 64}\n",
      "360    0    0  {-1: 360}\n",
      "  0   75  268  {3: 66, 1: 73, 0: 75, 2: 65, 4: 64}\n",
      "  0   81  308  {1: 68, 3: 84, 2: 71, 4: 85, 0: 81}\n",
      "  0   79  305  {0: 79, 1: 84, 2: 83, 4: 72, 3: 66}\n",
      "  0   74  295  {2: 67, 3: 82, 0: 74, 1: 80, 4: 66}\n",
      "  0   65  294  {0: 65, 1: 63, 3: 68, 2: 78, 4: 85}\n",
      "336    0    0  {-1: 336}\n",
      "\n",
      "\n",
      "\n",
      "[813, 812, 810, 812, 828]\n",
      "[-0.25, -0.37, -0.61, -0.37, 1.6]\n"
     ]
    }
   ],
   "source": [
    "import Functions.VideoUtils as v_utils\n",
    "import Functions.LocalDB as db\n",
    "import DATA_INFO as data_info\n",
    "import numpy as np\n",
    "frame_tale = 1\n",
    "augment_ratio = 2\n",
    "batch_size = 32\n",
    "stp_patience = 10\n",
    "class_num = 10\n",
    "epochs = 100\n",
    "folds = 5\n",
    "\n",
    "data_folder_name = 'T{}.C{}.V{}.A{}/'.format(1,class_num,20, augment_ratio)\n",
    "# cnn = CNN(class_num=class_num, frame_tale=frame_tale, augment_ratio=augment_ratio, batch_size=batch_size, \n",
    "#             early_stopping_patience=stp_patience, resize_images=(75, 75))    \n",
    "for fold in range(5):\n",
    "    # d_utils.update_K_Fold_Dataset(data_info.Labeled.path + cnn.dataset_folder_name, fold)\n",
    "    frames_info = db.get_db(data_info.Labeled.path + data_folder_name+'frames_info.txt', v_utils.frame_info)\n",
    "    vids = [{\"Folds\" : {}, \"val\" : 0, \"test\" : 0, \"train\" : 0} for i in range(14)]\n",
    "    for frame in frames_info:\n",
    "        x = v_utils.frame_info.cast(frame)\n",
    "        if(x.n_k_fold in list(vids[x.video_number][\"Folds\"].keys())):\n",
    "            vids[x.video_number][\"Folds\"][x.n_k_fold] = vids[x.video_number][\"Folds\"][x.n_k_fold] + 1\n",
    "        else:\n",
    "            vids[x.video_number][\"Folds\"][x.n_k_fold] = 1\n",
    "        vids[x.video_number][x.mode] = vids[x.video_number][x.mode] + 1\n",
    "\n",
    "    print(\"\\n\")\n",
    "    for vid in vids:\n",
    "        print(\"{:3d}  {:3d}  {:3d}  {}\".format(vid[\"test\"], vid[\"val\"], vid[\"train\"], vid[\"Folds\"]))\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    folds = [sum([v[\"Folds\"][f] for v in vids if 0 in v[\"Folds\"].keys()]) for f in range(folds)]\n",
    "    print(folds)\n",
    "    \n",
    "    print([round(100*(f-np.mean(folds))/np.mean(folds), 2) for f in folds])\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Get Prediction Run Time</h1>\n",
    "<h3>Converting to Frozen Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions.CNN import CNN\n",
    "# Converting to Frozen Model\n",
    "\n",
    "CNN.freeze_model('T1.C10.V20.A1.CONV.32.32.FULLY.32.32.h5', 'models/', 'frozen_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Predict Using Frozen Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from Functions.CNN import CNN\n",
    "import Functions.GlobalUtils as g_utils\n",
    "import Functions.DataUtils as d_utils\n",
    "import Functions.LocalDB as db\n",
    "import DATA_INFO as data_info\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import tensorflow as tf\n",
    "\n",
    "# Loading Prediction Data\n",
    "class_num = 10\n",
    "augment_ratio = 1\n",
    "data_folder_name = 'T{}.C{}.V{}.A{}/'.format(1,class_num,20, augment_ratio)\n",
    "labeled_data_info = db.get_db(data_info.Labeled.path + data_folder_name+'data_info.txt', d_utils.Labeled_data_info)\n",
    "test_indexes = labeled_data_info[0].test_videos_indexes\n",
    "folder = 'Predictions/'\n",
    "_borders = labeled_data_info[0].classes_borders\n",
    "_borders[0] = 0\n",
    "i = test_indexes[0]\n",
    "data_X, data_Y, real_flow = d_utils.single_vid_Data_Labeling(i,1, class_num, borders = _borders)\n",
    "data_X = np.reshape(data_X, (np.shape(data_X)[0], np.shape(data_X)[1],np.shape(data_X)[2], 1))\n",
    "\n",
    "# Loading model\n",
    "model_name = 'T1.C10.V20.A1.CONV.32.32.FULLY.32.32.pb'\n",
    "sess=tf.compat.v1.InteractiveSession()\n",
    "frozen_graph=\"./frozen_models/\" + model_name\n",
    "with tf.io.gfile.GFile(frozen_graph, \"rb\") as f:\n",
    "      graph_def = tf.compat.v1.GraphDef()\n",
    "      graph_def.ParseFromString(f.read())\n",
    "sess.graph.as_default()\n",
    "tf.import_graph_def(graph_def)\n",
    "input_tensor = sess.graph.get_tensor_by_name(\"x:0\") \n",
    "output_tensor = sess.graph.get_tensor_by_name(\"Identity:0\") \n",
    "timer = g_utils.timer()\n",
    "times\n",
    "for i in range(len(data_X)):\n",
    "      timer.start()\n",
    "      probs = sess.run(output_tensor, {'x:0': np.array([data_X[i]])})\n",
    "      _predicted = list(np.argmax(probs, axis=-1))\n",
    "      timer.stop()\n",
    "      times.append(timer.exact_passed_time)\n",
    "sess.close()\n",
    "print('\\n\\n')\n",
    "print('Average Runtime For Each Frame : ',timer.to_labeled_time(np.mean(times)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Post Proccess - Plot - Evaluate</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Functions.GlobalUtils as g_utils\n",
    "import numpy as np\n",
    "def postproccess(predicted, truth_flow, borders, f_min, f_max, xlim):\n",
    "        borders_denorm = [f_min] + list(g_utils.denormalize(borders[1:-1], f_max, f_min)) + [f_max]\n",
    "        data_denorm = g_utils.denormalize(truth_flow[int(xlim[0]*20):int(xlim[1]*20)], f_max, f_min)\n",
    "        pred_flow = [(borders_denorm[f]+borders_denorm[f+1]) / 2 for f in predicted[int(xlim[0]*20):int(xlim[1]*20)]]\n",
    "        pred_flow_pp = g_utils.apply_kernel(pred_flow, np.median, (7, 1))\n",
    "        pred_flow_pp = g_utils.apply_kernel(pred_flow_pp, np.mean, (7, 1))\n",
    "        pred_flow_pp = g_utils.apply_kernel(pred_flow_pp, np.mean, (7, 1))\n",
    "        \n",
    "        return pred_flow, pred_flow_pp, data_denorm, borders_denorm\n",
    "\n",
    "def limitation(predicted, data_denorm, xlim, borders_denorm):\n",
    "        target_data = predicted[int(xlim[0]*20):int(xlim[1]*20)]\n",
    "        min_data = [borders_denorm[d] for d in range(len(borders_denorm) - 1) if borders_denorm[d] < min(data_denorm) < borders_denorm[d+1]][-1]\n",
    "        mid = borders_denorm[borders_denorm.index(min_data)] + (borders_denorm[borders_denorm.index(min_data)+1] - borders_denorm[borders_denorm.index(min_data)])/2\n",
    "        if mid < min(data_denorm): min_data = mid\n",
    "        max_data = [borders_denorm[d+1] for d in range(len(borders_denorm) - 1) if borders_denorm[d] < max(data_denorm) < borders_denorm[d+1]][0]\n",
    "        mid = borders_denorm[borders_denorm.index(max_data)] - (borders_denorm[borders_denorm.index(max_data)] - borders_denorm[borders_denorm.index(max_data)-1])/2\n",
    "        if mid > max(data_denorm): max_data = mid\n",
    "        min_pred = borders_denorm[min(target_data)]\n",
    "        max_pred = borders_denorm[max(target_data) + 1]\n",
    "        time = np.arange(start=xlim[0], stop=xlim[1], step=1/20)\n",
    "        ylim = [min(min_data, min_pred), max(max_data, max_pred)]\n",
    "        return time, ylim\n",
    "\n",
    "def plot_table(dict, folds, test_videos_count):\n",
    "        decimals = 3\n",
    "        data = np.array(list(dict.values()))\n",
    "        data = data.reshape(len(dict.keys()), len(folds), test_videos_count)\n",
    "        mean = np.mean(data, axis=2)\n",
    "        mean = np.round(mean, decimals=decimals)\n",
    "        mean_overall = np.mean(mean, axis=1)\n",
    "        mean_overall = np.round(mean_overall, decimals=decimals)\n",
    "        # metric_names = ['F1 (%)', 'Corr (%)', 'R² (%)', 'MAE (%)', 'MSE']\n",
    "        metric_names = list(dict.keys())\n",
    "        print(\"{:<10}\".format(\"Metric\"), end='')\n",
    "        print((\" | {:<10}\"*len(folds)).format(*([\"Fold \" + str(f) for f in folds])), end='')\n",
    "        print(\" | {:<10}\".format(\"Avg\"))\n",
    "\n",
    "        print(\"{:<10}\".format(\"----------\"), end='')\n",
    "        print((\" | {:<10}\"*len(folds)).format(*([\"----------\"]*len(folds))), end='')\n",
    "        print(\" | {:<10}\".format(\"----------\"))\n",
    "\n",
    "        for i in range(data.shape[0]):\n",
    "                print(\"{:<10}\".format(metric_names[i]), end='')\n",
    "                for f in range(len(folds)):\n",
    "                        print(\" | {:<10.{}}\".format(mean[i][f], 8), end='')\n",
    "                print(\" | {:<10.{}}\".format(mean_overall[i], 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Person number 1\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "db not found! (path : Predictions/F0_C10_DenseNet121.h5.predict.txt)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Uni\\Project\\Main\\Test Predicrt.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Uni/Project/Main/Test%20Predicrt.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mfor\u001b[39;00m fold \u001b[39min\u001b[39;00m folds:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Uni/Project/Main/Test%20Predicrt.ipynb#X11sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     _file_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfolder\u001b[39m}\u001b[39;00m\u001b[39mF\u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m_C10_\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m.h5.predict.txt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Uni/Project/Main/Test%20Predicrt.ipynb#X11sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     data \u001b[39m=\u001b[39m db\u001b[39m.\u001b[39;49mget_db(_file_name, CNN\u001b[39m.\u001b[39;49mprediction_data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Uni/Project/Main/Test%20Predicrt.ipynb#X11sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     pp_flow_list \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Uni/Project/Main/Test%20Predicrt.ipynb#X11sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     class_corrs \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Uni\\Project\\Main\\Functions\\LocalDB.py:323\u001b[0m, in \u001b[0;36mget_db\u001b[1;34m(path, _class)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_db\u001b[39m(path, _class):\n\u001b[0;32m    322\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(path):\n\u001b[1;32m--> 323\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mdb not found! (path : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(path))\n\u001b[0;32m    325\u001b[0m     _db_info \u001b[39m=\u001b[39m get_db_info(path)\n\u001b[0;32m    327\u001b[0m     \u001b[39m# checking order of parameteres in the passed class\u001b[39;00m\n",
      "\u001b[1;31mException\u001b[0m: db not found! (path : Predictions/F0_C10_DenseNet121.h5.predict.txt)"
     ]
    }
   ],
   "source": [
    "with g_utils.Section_Separator('imports'):\n",
    "    import Functions.GlobalUtils as g_utils\n",
    "    import Functions.DataUtils as d_utils\n",
    "    import DATA_INFO as data_info\n",
    "    import Functions.LocalDB as db\n",
    "    from Functions.CNN import CNN\n",
    "    import numpy as np\n",
    "    import sklearn.metrics as metrics\n",
    "\n",
    "\n",
    "models = [\"DenseNet121\", \"EfficientNetB0\", \"InceptionV3\"]\n",
    "\n",
    "for model_name in models:\n",
    "    with g_utils.Section_Separator('initials'):\n",
    "            folds = range(5)\n",
    "            class_num = 10\n",
    "            augment_ratio = 1\n",
    "            folder = 'Predictions/'\n",
    "            fps = 20\n",
    "            f_max, f_min = d_utils.get_data_max_min(data_info.Trimmed.path, 'flow', '{:02d}data.txt')\n",
    "            correlation_total = np.array([])\n",
    "            maes_total = np.array([])\n",
    "            r_square_total = np.array([])\n",
    "            mse_total = np.array([])\n",
    "            rmse_total = np.array([])\n",
    "            f1_score_total = np.array([])\n",
    "            \n",
    "    _test_indexes = [1, 7, 13]\n",
    "    _view_videos = [0,1,2]\n",
    "    for j in range(3):\n",
    "        test_indexes = [_test_indexes[j]]\n",
    "        view_videos = [_view_videos[j]]\n",
    "        with g_utils.Section_Separator('initials'):\n",
    "            folds = range(5)\n",
    "            class_num = 10\n",
    "            augment_ratio = 1\n",
    "            folder = 'Predictions/'\n",
    "            fps = 20\n",
    "            f_max, f_min = d_utils.get_data_max_min(data_info.Trimmed.path, 'flow', '{:02d}data.txt')\n",
    "            correlation = np.array([])\n",
    "            maes = np.array([])\n",
    "            r_square = np.array([])\n",
    "            mse = np.array([])\n",
    "            rmse = np.array([])\n",
    "            f1_score = np.array([])\n",
    "        for fold in folds:\n",
    "            _file_name = f'{folder}{model_name}/F{fold}_C10_{model_name}.h5.predict.txt'\n",
    "            data = db.get_db(_file_name, CNN.prediction_data)\n",
    "            pp_flow_list = []\n",
    "            class_corrs = []\n",
    "            class_maes = []\n",
    "            for i in view_videos:\n",
    "                xlim = [0, len(data[i].predicted)/20]\n",
    "                pred_flow, pred_flow_pp, data_denorm, borders_denorm = postproccess(data[i].predicted, data[i].truth_flow, data[i].borders, f_min, f_max, xlim)\n",
    "                time, ylim = limitation(data[i].predicted, data_denorm, xlim, borders_denorm)\n",
    "                \n",
    "                pp_flow_list.append(pred_flow_pp)\n",
    "                \n",
    "                # with g_utils.Section_Separator('plots'):\n",
    "                #     p = 0\n",
    "                #     with g_utils.Section_Separator('post process'):\n",
    "                #         plt.figure(figsize=(12, 6))\n",
    "                #         plt.plot(time, data_denorm, linewidth=1, label='Flow', c='tab:blue')\n",
    "                #         plt.plot(time, pred_flow, linewidth=2, label='Prediction', c='tab:gray')\n",
    "                #         plt.plot(time, pred_flow_pp, label='Post Processed Prediction', c='tab:red', linestyle='dashed')\n",
    "                #         plt.title('Flow vs Time, Video Number {}, Final Post Processed Output'.format(test_indexes[i]))\n",
    "                #         plt.yticks(borders_denorm) ;plt.xlim(xlim) ;plt.ylim(ylim) ;plt.grid() ;plt.legend()\n",
    "                #         plt.xlabel('Time (s)') ;plt.ylabel('Flow (L/Min)') ;plt.tight_layout()\n",
    "                #         # plt.savefig(folder + 'Pred_F{}_C{}_{:02d}_{}_pp_final.png'.format(fold,class_num,i,p), dpi=500)\n",
    "                #         plt.show()\n",
    "                #         p = p + 1\n",
    "                #         pass\n",
    "                #     with g_utils.Section_Separator('final'):\n",
    "                #         plt.figure(figsize=(12, 6))\n",
    "                #         plt.plot(time, data_denorm, linewidth=1, label='Flow', c='tab:blue')\n",
    "                #         plt.plot(time, pred_flow_pp, label='Post Processed Prediction', c='tab:red')\n",
    "                #         plt.title('Flow vs Time, Video Number {}, Final Flow Predictions'.format(test_indexes[i]))\n",
    "                #         plt.yticks(borders_denorm) ;plt.xlim(xlim) ;plt.grid() ;plt.legend()\n",
    "                #         plt.xlabel('Time (s)') ;plt.ylabel('Flow (L/Min)') ;plt.tight_layout()\n",
    "                #         # plt.savefig(folder + 'Pred_C{}_{:02d}_{}_pp.png'.format(class_num,i,p), dpi=500)\n",
    "                #         plt.show()\n",
    "                #         p = p + 1\n",
    "                #         pass          \n",
    "                #     pass\n",
    "                \n",
    "                r_square = np.append(r_square, metrics.r2_score(data_denorm, pred_flow_pp))\n",
    "                correlation = np.append(correlation, g_utils.corr_np(data_denorm, pred_flow_pp))\n",
    "                maes = np.append(maes, g_utils.mae(data_denorm, pred_flow_pp, [f_max, f_min]))\n",
    "                mse = np.append(mse, metrics.mean_squared_error(g_utils.normalize(data_denorm,target_range=[0.5,-0.5]), g_utils.normalize(pred_flow_pp,target_range=[0.5,-0.5])))\n",
    "                rmse = np.append(rmse, metrics.mean_squared_error(g_utils.normalize(data_denorm,target_range=[0.5,-0.5]), g_utils.normalize(pred_flow_pp,target_range=[0.5,-0.5]),squared=False))\n",
    "                f1_score = np.append(f1_score, metrics.f1_score(data[i].predicted, data[i].truth_class, average='macro'))\n",
    "        \n",
    "        \n",
    "        correlation = correlation*100\n",
    "        maes = maes*100\n",
    "        r_square = r_square*100\n",
    "        f1_score = f1_score*100\n",
    "        \n",
    "        r_square_total = np.array(list(r_square_total)+ list(r_square))\n",
    "        correlation_total = np.array(list(correlation_total)+ list(correlation))\n",
    "        maes_total = np.array(list(maes_total)+ list(maes))\n",
    "        mse_total = np.array(list(mse_total)+ list(mse))\n",
    "        rmse_total = np.array(list(rmse_total)+ list(rmse))\n",
    "        f1_score_total = np.array(list(f1_score_total)+ list(f1_score))\n",
    "        \n",
    "        # print(f'\\nPerson number {j+1}')        \n",
    "        # plot_table({'F1 (%)' : f1_score, 'Corr (%)' : correlation, 'R² (%)' : r_square, 'MAE (%)' : maes, 'MSE' : mse, 'RMSE' : rmse}, folds, len(test_indexes))\n",
    "\n",
    "    print(f'\\nTotal {model_name} : ')\n",
    "    plot_table({'F1 (%)' : f1_score_total, 'Corr (%)' : correlation_total, 'R² (%)' : r_square_total, 'MAE (%)' : maes_total, 'MSE' : mse_total, 'RMSE' : rmse_total}, folds, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Get Post Proccess Run Time</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Functions.GlobalUtils as g_utils\n",
    "import Functions.DataUtils as d_utils\n",
    "import DATA_INFO as data_info\n",
    "import Functions.LocalDB as db\n",
    "from Functions.CNN import CNN\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "times = []\n",
    "class_num = 10\n",
    "frame_tale = 1\n",
    "validation_ratio = 0.2\n",
    "folder = 'Predictions/'\n",
    "arch = 'CONV.64.64.64.64.64.64.FULLY.64.64.64.64.64.64'\n",
    "_file_name = folder + 'T{}.C{}.V20.A5.{}.h5.predict.txt'.format(\n",
    "    frame_tale, class_num, arch)\n",
    "\n",
    "f_max, f_min = d_utils.get_data_max_min(\n",
    "    data_info.Trimmed.path, 'flow', '{:02d}data.txt')\n",
    "data = db.get_db(_file_name, CNN.prediction_data)\n",
    "fps = 20\n",
    "for i in [0, 1, 2]:\n",
    "    borders_denorm = [f_min] + list(g_utils.denormalize(data[i].borders[1:-1], f_max, f_min)) + [f_max]\n",
    "    \n",
    "    \n",
    "    timer = g_utils.timer()\n",
    "    timer.start()\n",
    "    pred_flow = [(borders_denorm[f]+borders_denorm[f+1]) /2 for f in data[i].predicted]\n",
    "    time = np.arange(start=0, stop=len(pred_flow)/fps, step=1/fps)\n",
    "    pred_flow_pp = g_utils.apply_kernel(pred_flow, np.median, (7, 1))\n",
    "    pred_flow_pp = g_utils.apply_kernel(pred_flow_pp, np.mean, (7, 1))\n",
    "    pred_flow_pp = g_utils.apply_kernel(pred_flow_pp, np.mean, (7, 1))\n",
    "    pred_flow_pp = g_utils.apply_kernel(pred_flow_pp, np.mean, (7, 1))\n",
    "    timer.stop()\n",
    "    times.append(timer.exact_passed_time/len(pred_flow_pp))\n",
    "\n",
    "print(times)\n",
    "print('\\n', 'Average Post Process Time : ', timer.to_labeled_time(np.mean(times)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
